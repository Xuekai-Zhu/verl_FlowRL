

@GPUMemoryLogger(role="flowrl actor", logger=None)
    def update_policy(self, data: DataProto):
        """Update policy using FlowRL trajectory balance loss."""
        # make sure we are in training mode
        self.actor_module.train()

        temperature = data.meta_info["temperature"]

        select_keys = [
            "responses",
            "response_mask",
            "input_ids",
            "attention_mask",
            "position_ids",
            "old_log_probs",
            "advantages",
            "ref_log_prob",  # FlowRL requires reference log probs
        ]
        if self.config.tis_imp_ratio_cap > 0:
            select_keys.append("rollout_log_probs")

        has_multi_modal_inputs = "multi_modal_inputs" in data.non_tensor_batch.keys()
        non_tensor_select_keys = ["multi_modal_inputs"] if has_multi_modal_inputs else []

        data = data.select(batch_keys=select_keys, non_tensor_batch_keys=non_tensor_select_keys)

        # Split to make minibatch iterator
        mini_batches = data.split(self.config.ppo_mini_batch_size)

        on_policy = len(mini_batches) == 1 and self.config.ppo_epochs == 1

        metrics = {}
        for _ in range(self.config.ppo_epochs):
            for batch_idx, mini_batch in enumerate(mini_batches):
                if self.config.use_dynamic_bsz:
                    max_token_len = self.config.ppo_max_token_len_per_gpu * self.ulysses_sequence_parallel_size
                    micro_batches, _ = prepare_dynamic_batch(mini_batch, max_token_len=max_token_len)
                else:
                    self.gradient_accumulation = (
                        self.config.ppo_mini_batch_size // self.config.ppo_micro_batch_size_per_gpu
                    )
                    micro_batches = mini_batch.split(self.config.ppo_micro_batch_size_per_gpu)

                self.actor_optimizer.zero_grad()

                for micro_batch in micro_batches:
                    micro_batch = micro_batch.to(get_device_id())
                    micro_batch_metrics = {}
                    model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}
                    response_mask = model_inputs["response_mask"]
                    old_log_prob = model_inputs["old_log_probs"]
                    advantages = model_inputs["advantages"]
                    ref_log_prob = model_inputs["ref_log_prob"]

                    loss_agg_mode = self.config.loss_agg_mode

                    if self.config.use_dynamic_bsz:
                        loss_scale_factor = response_mask.shape[0] / self.config.ppo_mini_batch_size
                    else:
                        loss_scale_factor = 1 / self.gradient_accumulation

                    # Forward pass with log_z computation
                    entropy, log_prob, log_z = self._forward_micro_batch(
                        model_inputs, temperature=temperature, calculate_entropy=False, return_log_z=True
                    )

                    if on_policy:
                        old_log_prob = log_prob.detach()

                    # Compute FlowRL trajectory balance loss
                    policy_loss, flowrl_metrics = self.compute_flowrl_objective(
                        logpf=log_prob,
                        logf_ref=ref_log_prob,
                        logpf_old=old_log_prob,
                        log_z=log_z,
                        reward=advantages,
                        response_mask=response_mask,
                        clip_ratio=self.config.clip_ratio
                    )

                    if self.config.use_dynamic_bsz:
                        loss = policy_loss * loss_scale_factor
                    else:
                        loss = policy_loss * loss_scale_factor
                    loss.backward()

                    micro_batch_metrics.update(flowrl_metrics)
                    for key in micro_batch_metrics:
                        if key != "actor/importance_weight":  # don't scale importance weight metric
                            micro_batch_metrics[key] *= loss_scale_factor

                    append_to_dict(metrics, micro_batch_metrics)

                grad_norm = self._optimizer_step()
                mini_batch_metrics = {"actor/grad_norm": grad_norm.detach().item()}
                append_to_dict(metrics, mini_batch_metrics)

        self.actor_optimizer.zero_grad()
        return metrics